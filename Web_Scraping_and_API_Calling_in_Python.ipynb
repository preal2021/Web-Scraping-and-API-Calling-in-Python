{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping and API Calling in Python**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Setup and Importing Libraries**\n",
        "We need to install and import the necessary Python libraries for web scraping and API calls:\n",
        "- **`requests`**: For making HTTP requests to websites and APIs.\n",
        "- **`BeautifulSoup`**: For parsing and navigating HTML data.\n",
        "- **`pandas`**: For data manipulation and exporting data.\n",
        "- **`time`**: To introduce delays between requests.\n",
        "\n"
      ],
      "metadata": {
        "id": "LhZNIpfUDerz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmIoI5apDd81",
        "outputId": "4a4db5d7-7d5b-4eda-972b-b085ffe287b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries (run this only if the libraries are not already installed)\n",
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "fRWBeBwpDzxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Real-World Web Scraping Example: IMDb Top 250 Movies**\n",
        "\n",
        "We’ll scrape the **IMDb Top 250 Movies** to fetch:\n",
        "- Movie Titles\n",
        "- Release Years\n",
        "- IMDb Ratings"
      ],
      "metadata": {
        "id": "JEcYNv-gEBvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Import library to make HTTP requests\n",
        "from bs4 import BeautifulSoup  # Import library to parse HTML\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "\n",
        "def scrape_imdb_top_250():  # Define main scraping function\n",
        "    url = \"https://www.imdb.com/chart/top/\"  # Define IMDb top movies URL\n",
        "\n",
        "    # Set headers to mimic browser request and avoid being blocked\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Send GET request to URL with specified headers and timeout\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        # Raise exception for bad HTTP responses\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Select all movie list items using CSS selector\n",
        "        rows = soup.select('ul.ipc-metadata-list li.ipc-metadata-list-summary-item')\n",
        "\n",
        "        movies = []  # Initialize empty list to store movie data\n",
        "        for row in rows[:250]:  # Iterate through first 250 movies\n",
        "            # Extract movie title, removing ranking number\n",
        "            title_elem = row.select_one('h3.ipc-title__text')\n",
        "            title = title_elem.text.split('. ', 1)[1] if title_elem else \"N/A\"\n",
        "\n",
        "            # Extract movie year from metadata\n",
        "            year_elem = row.select_one('.cli-title-metadata .cli-title-metadata-item')\n",
        "            year = year_elem.text if year_elem else \"N/A\"\n",
        "\n",
        "            # Extract movie rating\n",
        "            rating_elem = row.select_one('span.ipc-rating-star--imdb')\n",
        "            rating = rating_elem.text.split()[0] if rating_elem else \"N/A\"\n",
        "\n",
        "            # Append movie details to movies list\n",
        "            movies.append({\n",
        "                'Title': title,\n",
        "                'Year': year,\n",
        "                'Rating': rating\n",
        "            })\n",
        "\n",
        "        # Convert movies list to pandas DataFrame\n",
        "        movies_df = pd.DataFrame(movies)\n",
        "        # Save DataFrame to CSV file\n",
        "        movies_df.to_csv('imdb_top_250.csv', index=False)\n",
        "\n",
        "        return movies_df\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        # Print error message if request fails\n",
        "        print(f\"Error fetching IMDb data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Run the scraper if script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    top_movies = scrape_imdb_top_250()  # Call scraping function\n",
        "    print(\"Top 5 Movies:\")  # Print first 5 movies\n",
        "    print(top_movies.head())\n",
        "    print(\"Data saved to imdb_top_250.csv\")  # Confirm CSV save"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFu6IX12EOTI",
        "outputId": "d03ac328-5253-45f9-a9aa-38b9f131d823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Movies:\n",
            "                      Title  Year Rating\n",
            "0  The Shawshank Redemption  1994    9.3\n",
            "1             The Godfather  1972    9.2\n",
            "2           The Dark Knight  2008    9.0\n",
            "3     The Godfather Part II  1974    9.0\n",
            "4          De 12 gezworenen  1957    9.0\n",
            "Data saved to imdb_top_250.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Real-World Web Scraping Example: Books to Scrape**\n",
        "We’ll scrape data from **Books to Scrape**:\n",
        "- Book Titles\n",
        "- Prices\n",
        "- Availability Status"
      ],
      "metadata": {
        "id": "dzf6F2ytEiHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL for the \"Fiction\" books category\n",
        "books_url = \"http://books.toscrape.com/catalogue/category/books/fiction_10/index.html\"\n",
        "\n",
        "# Fetch webpage content\n",
        "response = requests.get(books_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract book details\n",
        "books = []\n",
        "for book in soup.select('article.product_pod'):\n",
        "    title = book.h3.a['title']\n",
        "    price = book.select_one('.price_color').text\n",
        "    availability = book.select_one('.availability').text.strip()\n",
        "    books.append({'Title': title, 'Price': price, 'Availability': availability})\n",
        "\n",
        "# Save to DataFrame\n",
        "books_df = pd.DataFrame(books)\n",
        "print(\"Sample Data:\")\n",
        "print(books_df.head())\n",
        "\n",
        "# Save to CSV\n",
        "books_df.to_csv('books_data.csv', index=False)\n",
        "print(\"Books data saved to books_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZt50BPsErgV",
        "outputId": "229dbcdd-e326-4a41-e73b-ff23f8f9c53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Data:\n",
            "                                               Title   Price Availability\n",
            "0                                         Soumission  £50.10     In stock\n",
            "1                        Private Paris (Private #10)  £47.61     In stock\n",
            "2                       We Love You, Charlie Freeman  £50.27     In stock\n",
            "3                                             Thirst  £17.27     In stock\n",
            "4  The Murder That Never Was (Forensic Instincts #5)  £54.11     In stock\n",
            "Books data saved to books_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Real-World API Example: OpenWeatherMap**\n",
        "We will use the **OpenWeatherMap API** to fetch:\n",
        "- Real-time weather data for cities like London, New York, and Tokyo.\n",
        "- Data includes Temperature, Weather Description, and Humidity.\n",
        "\n",
        "**Note:** Replace `\"your_api_key_here\"` with your OpenWeatherMap API Key."
      ],
      "metadata": {
        "id": "zxlonIROE0nO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EK2y71zEP4On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Import library for making HTTP requests\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from google.colab import userdata # Import the userdata module to access user secrets.\n",
        "\n",
        "\n",
        "def get_coordinates(city):\n",
        "    \"\"\"Geocoding API to convert city names to coordinates\"\"\"\n",
        "    # Define Geocoding API endpoint\n",
        "    geocode_url = \"http://api.openweathermap.org/geo/1.0/direct\"\n",
        "    # API key for authentication\n",
        "    api_key = userdata.get('OPENWEATHERMAP_API_KEY')\n",
        "\n",
        "    # Parameters for geocoding request\n",
        "    params = {\n",
        "        'q': city,  # City name\n",
        "        'limit': 1,  # Limit to first result\n",
        "        'appid': api_key  # API authentication\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Send GET request to geocoding API\n",
        "        response = requests.get(geocode_url, params=params)\n",
        "        # Raise exception for bad HTTP responses\n",
        "        response.raise_for_status()\n",
        "        # Parse JSON response\n",
        "        data = response.json()\n",
        "\n",
        "        # Check if coordinates found\n",
        "        if data:\n",
        "            # Return dictionary with latitude and longitude\n",
        "            return {\n",
        "                'lat': data[0]['lat'],\n",
        "                'lon': data[0]['lon']\n",
        "            }\n",
        "        else:\n",
        "            # Print error if no coordinates found\n",
        "            print(f\"No coordinates found for {city}\")\n",
        "            return None\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        # Handle any request-related errors\n",
        "        print(f\"Geocoding error for {city}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_weather_data(cities):\n",
        "    \"\"\"Fetch weather data for given cities using coordinates\"\"\"\n",
        "    # Weather API endpoint\n",
        "    weather_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
        "    # API key\n",
        "    api_key = userdata.get('OPENWEATHERMAP_API_KEY')\n",
        "\n",
        "    # List to store weather data\n",
        "    weather_data = []\n",
        "\n",
        "    # Iterate through each city\n",
        "    for city in cities:\n",
        "        # Get coordinates for the city\n",
        "        coords = get_coordinates(city)\n",
        "\n",
        "        # Proceed if coordinates found\n",
        "        if coords:\n",
        "            # Parameters for weather API request\n",
        "            params = {\n",
        "                'lat': coords['lat'],  # Latitude\n",
        "                'lon': coords['lon'],  # Longitude\n",
        "                'appid': api_key,  # API authentication\n",
        "                'units': 'metric'  # Temperature in Celsius\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Send GET request to weather API\n",
        "                response = requests.get(weather_url, params=params)\n",
        "                # Raise exception for bad HTTP responses\n",
        "                response.raise_for_status()\n",
        "                # Parse JSON response\n",
        "                data = response.json()\n",
        "\n",
        "                # Append weather data to list\n",
        "                weather_data.append({\n",
        "                    'City': city,\n",
        "                    'Latitude': coords['lat'],\n",
        "                    'Longitude': coords['lon'],\n",
        "                    'Temperature (C)': data['main']['temp'],\n",
        "                    'Feels Like (C)': data['main']['feels_like'],\n",
        "                    'Weather': data['weather'][0]['description'],\n",
        "                    'Humidity (%)': data['main']['humidity'],\n",
        "                    'Wind Speed (m/s)': data['wind']['speed']\n",
        "                })\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                # Handle any request-related errors\n",
        "                print(f\"Weather data error for {city}: {e}\")\n",
        "\n",
        "    # Return collected weather data\n",
        "    return weather_data\n",
        "\n",
        "def main():\n",
        "    # List of cities to fetch weather for\n",
        "    cities = [\"London\", \"New York\", \"Tokyo\"]\n",
        "\n",
        "    # Fetch weather data for cities\n",
        "    weather_data = fetch_weather_data(cities)\n",
        "\n",
        "    # Process and save data if retrieved\n",
        "    if weather_data:\n",
        "        # Convert to pandas DataFrame\n",
        "        weather_df = pd.DataFrame(weather_data)\n",
        "        # Print weather data\n",
        "        print(\"Weather Data:\")\n",
        "        print(weather_df)\n",
        "\n",
        "        # Save to CSV file\n",
        "        weather_df.to_csv('weather_data.csv', index=False)\n",
        "        print(\"Weather data saved to weather_data.csv\")\n",
        "    else:\n",
        "        # Print message if no data retrieved\n",
        "        print(\"No weather data retrieved.\")\n",
        "\n",
        "# Run main function if script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdG8OiVXEyL0",
        "outputId": "2fc3c995-0268-4193-f576-eb63420e6237"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather Data:\n",
            "       City   Latitude   Longitude  Temperature (C)  Feels Like (C)  \\\n",
            "0    London  51.507322   -0.127647             8.46            6.58   \n",
            "1  New York  40.712728  -74.006015             1.32           -4.50   \n",
            "2     Tokyo  35.682839  139.759455             9.39            7.45   \n",
            "\n",
            "            Weather  Humidity (%)  Wind Speed (m/s)  \n",
            "0   overcast clouds            78              3.13  \n",
            "1  scattered clouds            40              7.72  \n",
            "2         clear sky            53              3.60  \n",
            "Weather data saved to weather_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Handling Errors in API Requests**\n",
        "\n",
        "Error handling is critical for robust programs. Here, we handle:\n",
        "- **Network issues**: Using `try-except` blocks around HTTP requests.\n",
        "- **Invalid responses**: Using `response.raise_for_status()` to catch HTTP errors.\n",
        "- **No data**: Checking if the API returns valid data before proceeding."
      ],
      "metadata": {
        "id": "wp7Zjg6aFIFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Example with an invalid city\n",
        "    city = \"InvalidCity\"\n",
        "    params = {'q': city, 'appid': \"35f138bba66701b823bd50c63c850e7d\"}\n",
        "    response = requests.get(\"http://api.openweathermap.org/geo/1.0/direct\", params=params)\n",
        "    response.raise_for_status()  # Raise an error for bad HTTP responses\n",
        "\n",
        "    data = response.json()\n",
        "    if not data:\n",
        "        print(f\"No data found for city: {city}\")\n",
        "    else:\n",
        "        print(f\"Geocoding data for {city}: {data}\")\n",
        "except requests.exceptions.HTTPError as http_err:\n",
        "    print(f\"HTTP error occurred: {http_err}\")\n",
        "except requests.exceptions.RequestException as req_err:\n",
        "    print(f\"Request error occurred: {req_err}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsm4aJ_4FN_s",
        "outputId": "2affe043-54c7-4e60-ac41-315ac6571c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data found for city: InvalidCity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Respecting API Rate Limits and Adding Delays**\n",
        "\n",
        "When making repeated requests (e.g., to scrape multiple pages or fetch data for multiple cities), respect rate limits using the `time.sleep()` function to introduce delays.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j90UlA0kFYDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Example with delays\n",
        "cities = [\"London\", \"New York\", \"Tokyo\"]\n",
        "for city in cities:\n",
        "    print(f\"Fetching weather data for {city}...\")\n",
        "    # Add a delay of 2 seconds between API calls\n",
        "    time.sleep(2)\n",
        "    # Fetch weather data (code here calls the API functions written earlier)\n",
        "    print(f\"Data for {city} fetched!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgxBjykFlNQ",
        "outputId": "8c450e51-7dee-4856-9240-7c0ca99ecb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching weather data for London...\n",
            "Data for London fetched!\n",
            "Fetching weather data for New York...\n",
            "Data for New York fetched!\n",
            "Fetching weather data for Tokyo...\n",
            "Data for Tokyo fetched!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Exporting and Analyzing the Data**\n",
        "\n",
        "\n",
        "The retrieved weather data can be saved in a CSV file for further analysis using tools like Pandas. This allows us to:\n",
        "- Store structured data for future use.\n",
        "- Analyze or visualize the data with other Python libraries or BI tools.\n",
        "\n"
      ],
      "metadata": {
        "id": "LzPqrnxWFtEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and display the saved weather data\n",
        "try:\n",
        "    weather_df = pd.read_csv('weather_data.csv')\n",
        "    print(\"Loaded Weather Data:\")\n",
        "    print(weather_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Weather data CSV file not found. Run the script to generate it first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XWjaZe1F866",
        "outputId": "a560884e-9408-434e-949f-2e6acd4acbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Weather Data:\n",
            "       City   Latitude   Longitude  Temperature (C)  Feels Like (C)  \\\n",
            "0    London  51.507322   -0.127647             8.35            6.47   \n",
            "1  New York  40.712728  -74.006015             1.24           -4.61   \n",
            "2     Tokyo  35.682839  139.759455             9.68            7.80   \n",
            "\n",
            "            Weather  Humidity (%)  Wind Speed (m/s)  \n",
            "0   overcast clouds            76              3.09  \n",
            "1  scattered clouds            40              7.72  \n",
            "2         clear sky            53              3.60  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Conclusion**\n",
        "\n",
        "#### **Text Cell**:\n",
        "**Key Takeaways**:\n",
        "- **Web Scraping**: Extract structured data using `requests` and `BeautifulSoup`.\n",
        "- **API Calling**: Use `requests` to interact with RESTful APIs.\n",
        "- **Error Handling**: Catch and handle common errors to make your program robust.\n",
        "- **Rate Limits**: Respect API usage guidelines with delays.\n",
        "- **Data Export**: Save data in CSV format for further use and analysis.\n",
        "\n",
        "This notebook provided examples using:\n",
        "1. The **OpenWeatherMap API** for weather and geocoding data.\n",
        "2. **Error handling** and **rate limits** for safe and efficient API use.\n"
      ],
      "metadata": {
        "id": "KDvskW7WGFmx"
      }
    }
  ]
}