{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhZNIpfUDerz"
   },
   "source": [
    "# **Web Scraping and API Calling in Python**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Setup and Importing Libraries**\n",
    "We need to install and import the necessary Python libraries for web scraping and API calls:\n",
    "- **`requests`**: For making HTTP requests to websites and APIs.\n",
    "- **`BeautifulSoup`**: For parsing and navigating HTML data.\n",
    "- **`pandas`**: For data manipulation and exporting data.\n",
    "- **`time`**: To introduce delays between requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmIoI5apDd81",
    "outputId": "4a4db5d7-7d5b-4eda-972b-b085ffe287b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prathamesh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries (run this only if the libraries are not already installed)\n",
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fRWBeBwpDzxA"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEcYNv-gEBvh"
   },
   "source": [
    "### **2. Real-World Web Scraping Example: IMDb Top 250 Movies**\n",
    "\n",
    "We’ll scrape the **IMDb Top 250 Movies** to fetch:\n",
    "- Movie Titles\n",
    "- Release Years\n",
    "- IMDb Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFu6IX12EOTI",
    "outputId": "d03ac328-5253-45f9-a9aa-38b9f131d823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Movies:\n",
      "                      Title  Year Rating\n",
      "0  The Shawshank Redemption  1994    9.3\n",
      "1             The Godfather  1972    9.2\n",
      "2           The Dark Knight  2008    9.0\n",
      "3    The Godfather: Part II  1974    9.0\n",
      "4              12 Angry Men  1957    9.0\n",
      "Data saved to imdb_top_250.csv\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import library to make HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import library to parse HTML\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "\n",
    "def scrape_imdb_top_250():  # Define main scraping function\n",
    "    url = \"https://www.imdb.com/chart/top/\"  # Define IMDb top movies URL\n",
    "\n",
    "    # Set headers to mimic browser request and avoid being blocked\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send GET request to URL with specified headers and timeout\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        # Raise exception for bad HTTP responses\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select all movie list items using CSS selector\n",
    "        rows = soup.select('ul.ipc-metadata-list li.ipc-metadata-list-summary-item')\n",
    "\n",
    "        movies = []  # Initialize empty list to store movie data\n",
    "        for row in rows[:250]:  # Iterate through first 250 movies\n",
    "            # Extract movie title, removing ranking number\n",
    "            title_elem = row.select_one('h3.ipc-title__text')\n",
    "            title = title_elem.text.split('. ', 1)[1] if title_elem else \"N/A\"\n",
    "\n",
    "            # Extract movie year from metadata\n",
    "            year_elem = row.select_one('.cli-title-metadata .cli-title-metadata-item')\n",
    "            year = year_elem.text if year_elem else \"N/A\"\n",
    "\n",
    "            # Extract movie rating\n",
    "            rating_elem = row.select_one('span.ipc-rating-star--imdb')\n",
    "            rating = rating_elem.text.split()[0] if rating_elem else \"N/A\"\n",
    "\n",
    "            # Append movie details to movies list\n",
    "            movies.append({\n",
    "                'Title': title,\n",
    "                'Year': year,\n",
    "                'Rating': rating\n",
    "            })\n",
    "\n",
    "        # Convert movies list to pandas DataFrame\n",
    "        movies_df = pd.DataFrame(movies)\n",
    "        # Save DataFrame to CSV file\n",
    "        movies_df.to_csv('imdb_top_250.csv', index=False)\n",
    "\n",
    "        return movies_df\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # Print error message if request fails\n",
    "        print(f\"Error fetching IMDb data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run the scraper if script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    top_movies = scrape_imdb_top_250()  # Call scraping function\n",
    "    print(\"Top 5 Movies:\")  # Print first 5 movies\n",
    "    print(top_movies.head())\n",
    "    print(\"Data saved to imdb_top_250.csv\")  # Confirm CSV save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzf6F2ytEiHd"
   },
   "source": [
    "### **3. Real-World Web Scraping Example: Books to Scrape**\n",
    "We’ll scrape data from **Books to Scrape**:\n",
    "- Book Titles\n",
    "- Prices\n",
    "- Availability Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZt50BPsErgV",
    "outputId": "229dbcdd-e326-4a41-e73b-ff23f8f9c53c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "                                               Title   Price Availability\n",
      "0                                         Soumission  £50.10     In stock\n",
      "1                        Private Paris (Private #10)  £47.61     In stock\n",
      "2                       We Love You, Charlie Freeman  £50.27     In stock\n",
      "3                                             Thirst  £17.27     In stock\n",
      "4  The Murder That Never Was (Forensic Instincts #5)  £54.11     In stock\n",
      "Books data saved to books_data.csv\n"
     ]
    }
   ],
   "source": [
    "# URL for the \"Fiction\" books category\n",
    "books_url = \"http://books.toscrape.com/catalogue/category/books/fiction_10/index.html\"\n",
    "\n",
    "# Fetch webpage content\n",
    "response = requests.get(books_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract book details\n",
    "books = []\n",
    "for book in soup.select('article.product_pod'):\n",
    "    title = book.h3.a['title']\n",
    "    price = book.select_one('.price_color').text\n",
    "    availability = book.select_one('.availability').text.strip()\n",
    "    books.append({'Title': title, 'Price': price, 'Availability': availability})\n",
    "\n",
    "# Save to DataFrame\n",
    "books_df = pd.DataFrame(books)\n",
    "print(\"Sample Data:\")\n",
    "print(books_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "books_df.to_csv('books_data.csv', index=False)\n",
    "print(\"Books data saved to books_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxlonIROE0nO"
   },
   "source": [
    "### **4. Real-World API Example: OpenWeatherMap**\n",
    "We will use the **OpenWeatherMap API** to fetch:\n",
    "- Real-time weather data for cities like London, New York, and Tokyo.\n",
    "- Data includes Temperature, Weather Description, and Humidity.\n",
    "\n",
    "**Note:** Replace `\"your_api_key_here\"` with your OpenWeatherMap API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdG8OiVXEyL0",
    "outputId": "2fc3c995-0268-4193-f576-eb63420e6237"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m  \u001b[38;5;66;03m# Import library for making HTTP requests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Import pandas for data manipulation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata \u001b[38;5;66;03m# Import the userdata module to access user secrets.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coordinates\u001b[39m(city):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Geocoding API to convert city names to coordinates\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import requests  # Import library for making HTTP requests\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from google.colab import userdata # Import the userdata module to access user secrets.\n",
    "\n",
    "\n",
    "def get_coordinates(city):\n",
    "    \"\"\"Geocoding API to convert city names to coordinates\"\"\"\n",
    "    # Define Geocoding API endpoint\n",
    "    geocode_url = \"http://api.openweathermap.org/geo/1.0/direct\"\n",
    "    # API key for authentication\n",
    "    api_key = userdata.get('OPENWEATHERMAP_API_KEY')\n",
    "\n",
    "    # Parameters for geocoding request\n",
    "    params = {\n",
    "        'q': city,  # City name\n",
    "        'limit': 1,  # Limit to first result\n",
    "        'appid': api_key  # API authentication\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send GET request to geocoding API\n",
    "        response = requests.get(geocode_url, params=params)\n",
    "        # Raise exception for bad HTTP responses\n",
    "        response.raise_for_status()\n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if coordinates found\n",
    "        if data:\n",
    "            # Return dictionary with latitude and longitude\n",
    "            return {\n",
    "                'lat': data[0]['lat'],\n",
    "                'lon': data[0]['lon']\n",
    "            }\n",
    "        else:\n",
    "            # Print error if no coordinates found\n",
    "            print(f\"No coordinates found for {city}\")\n",
    "            return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # Handle any request-related errors\n",
    "        print(f\"Geocoding error for {city}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_weather_data(cities):\n",
    "    \"\"\"Fetch weather data for given cities using coordinates\"\"\"\n",
    "    # Weather API endpoint\n",
    "    weather_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    # API key\n",
    "    api_key = userdata.get('OPENWEATHERMAP_API_KEY')\n",
    "\n",
    "    # List to store weather data\n",
    "    weather_data = []\n",
    "\n",
    "    # Iterate through each city\n",
    "    for city in cities:\n",
    "        # Get coordinates for the city\n",
    "        coords = get_coordinates(city)\n",
    "\n",
    "        # Proceed if coordinates found\n",
    "        if coords:\n",
    "            # Parameters for weather API request\n",
    "            params = {\n",
    "                'lat': coords['lat'],  # Latitude\n",
    "                'lon': coords['lon'],  # Longitude\n",
    "                'appid': api_key,  # API authentication\n",
    "                'units': 'metric'  # Temperature in Celsius\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Send GET request to weather API\n",
    "                response = requests.get(weather_url, params=params)\n",
    "                # Raise exception for bad HTTP responses\n",
    "                response.raise_for_status()\n",
    "                # Parse JSON response\n",
    "                data = response.json()\n",
    "\n",
    "                # Append weather data to list\n",
    "                weather_data.append({\n",
    "                    'City': city,\n",
    "                    'Latitude': coords['lat'],\n",
    "                    'Longitude': coords['lon'],\n",
    "                    'Temperature (C)': data['main']['temp'],\n",
    "                    'Feels Like (C)': data['main']['feels_like'],\n",
    "                    'Weather': data['weather'][0]['description'],\n",
    "                    'Humidity (%)': data['main']['humidity'],\n",
    "                    'Wind Speed (m/s)': data['wind']['speed']\n",
    "                })\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                # Handle any request-related errors\n",
    "                print(f\"Weather data error for {city}: {e}\")\n",
    "\n",
    "    # Return collected weather data\n",
    "    return weather_data\n",
    "\n",
    "def main():\n",
    "    # List of cities to fetch weather for\n",
    "    cities = [\"London\", \"New York\", \"Tokyo\"]\n",
    "\n",
    "    # Fetch weather data for cities\n",
    "    weather_data = fetch_weather_data(cities)\n",
    "\n",
    "    # Process and save data if retrieved\n",
    "    if weather_data:\n",
    "        # Convert to pandas DataFrame\n",
    "        weather_df = pd.DataFrame(weather_data)\n",
    "        # Print weather data\n",
    "        print(\"Weather Data:\")\n",
    "        print(weather_df)\n",
    "\n",
    "        # Save to CSV file\n",
    "        weather_df.to_csv('weather_data.csv', index=False)\n",
    "        print(\"Weather data saved to weather_data.csv\")\n",
    "    else:\n",
    "        # Print message if no data retrieved\n",
    "        print(\"No weather data retrieved.\")\n",
    "\n",
    "# Run main function if script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp7Zjg6aFIFe"
   },
   "source": [
    "### **5. Handling Errors in API Requests**\n",
    "\n",
    "Error handling is critical for robust programs. Here, we handle:\n",
    "- **Network issues**: Using `try-except` blocks around HTTP requests.\n",
    "- **Invalid responses**: Using `response.raise_for_status()` to catch HTTP errors.\n",
    "- **No data**: Checking if the API returns valid data before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsm4aJ_4FN_s",
    "outputId": "2affe043-54c7-4e60-ac41-315ac6571c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for city: InvalidCity\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Example with an invalid city\n",
    "    city = \"InvalidCity\"\n",
    "    api_key = userdata.get('OPENWEATHERMAP_API_KEY')\n",
    "    params = {'q': city, 'appid': api_key}\n",
    "    response = requests.get(\"http://api.openweathermap.org/geo/1.0/direct\", params=params)\n",
    "    response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "\n",
    "    data = response.json()\n",
    "    if not data:\n",
    "        print(f\"No data found for city: {city}\")\n",
    "    else:\n",
    "        print(f\"Geocoding data for {city}: {data}\")\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f\"Request error occurred: {req_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j90UlA0kFYDo"
   },
   "source": [
    "### **6. Respecting API Rate Limits and Adding Delays**\n",
    "\n",
    "When making repeated requests (e.g., to scrape multiple pages or fetch data for multiple cities), respect rate limits using the `time.sleep()` function to introduce delays.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rgxBjykFlNQ",
    "outputId": "8c450e51-7dee-4856-9240-7c0ca99ecb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather data for London...\n",
      "Data for London fetched!\n",
      "Fetching weather data for New York...\n",
      "Data for New York fetched!\n",
      "Fetching weather data for Tokyo...\n",
      "Data for Tokyo fetched!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Example with delays\n",
    "cities = [\"London\", \"New York\", \"Tokyo\"]\n",
    "for city in cities:\n",
    "    print(f\"Fetching weather data for {city}...\")\n",
    "    # Add a delay of 2 seconds between API calls\n",
    "    time.sleep(2)\n",
    "    # Fetch weather data (code here calls the API functions written earlier)\n",
    "    print(f\"Data for {city} fetched!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPqrnxWFtEb"
   },
   "source": [
    "### **7. Exporting and Analyzing the Data**\n",
    "\n",
    "\n",
    "The retrieved weather data can be saved in a CSV file for further analysis using tools like Pandas. This allows us to:\n",
    "- Store structured data for future use.\n",
    "- Analyze or visualize the data with other Python libraries or BI tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XWjaZe1F866",
    "outputId": "a560884e-9408-434e-949f-2e6acd4acbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Weather Data:\n",
      "       City   Latitude   Longitude  Temperature (C)  Feels Like (C)  \\\n",
      "0    London  51.507322   -0.127647             8.35            6.47   \n",
      "1  New York  40.712728  -74.006015             1.24           -4.61   \n",
      "2     Tokyo  35.682839  139.759455             9.68            7.80   \n",
      "\n",
      "            Weather  Humidity (%)  Wind Speed (m/s)  \n",
      "0   overcast clouds            76              3.09  \n",
      "1  scattered clouds            40              7.72  \n",
      "2         clear sky            53              3.60  \n"
     ]
    }
   ],
   "source": [
    "# Load and display the saved weather data\n",
    "try:\n",
    "    weather_df = pd.read_csv('weather_data.csv')\n",
    "    print(\"Loaded Weather Data:\")\n",
    "    print(weather_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Weather data CSV file not found. Run the script to generate it first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDvskW7WGFmx"
   },
   "source": [
    "### **8. Conclusion**\n",
    "\n",
    "#### **Text Cell**:\n",
    "**Key Takeaways**:\n",
    "- **Web Scraping**: Extract structured data using `requests` and `BeautifulSoup`.\n",
    "- **API Calling**: Use `requests` to interact with RESTful APIs.\n",
    "- **Error Handling**: Catch and handle common errors to make your program robust.\n",
    "- **Rate Limits**: Respect API usage guidelines with delays.\n",
    "- **Data Export**: Save data in CSV format for further use and analysis.\n",
    "\n",
    "This notebook provided examples using:\n",
    "1. The **OpenWeatherMap API** for weather and geocoding data.\n",
    "2. **Error handling** and **rate limits** for safe and efficient API use.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
